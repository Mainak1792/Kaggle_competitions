{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roton.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1agGH7uRKhLkTVqx8e73WdQSYQJkUpLlB",
      "authorship_tag": "ABX9TyN8N2QZ9cO+MhckidjOY6iX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mainak1792/Kaggle_competitions/blob/main/Custom_food_vegetable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvYL_lnnkvzU"
      },
      "source": [
        "Import The dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8byQd8Ak0HU"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "import matplotlib.image as img\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import collections\n",
        "from shutil import copy\n",
        "from shutil import copytree, rmtree\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "import cv2"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NChbynNdkwSE"
      },
      "source": [
        "Data Augmentaiton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KplOx7-Wk4IK",
        "outputId": "f4dddfac-57f9-422d-bdea-17ac884170a1"
      },
      "source": [
        "TRAINING_DIR = \"/content/drive/MyDrive/crop_Vegi/Train\"\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "\t    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "VALIDATION_DIR = \"/content/drive/MyDrive/crop_Vegi/Test\"\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "\tTRAINING_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=64\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\tVALIDATION_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=64\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3914 images belonging to 17 classes.\n",
            "Found 1065 images belonging to 17 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMxNj9xpkwnR"
      },
      "source": [
        "I am using Three models to classify:\n",
        "1. 4-Layer Custom CNN \n",
        "2. INCEPTION v3\n",
        "3. mOBILE NET V\n",
        "4. RESNET 50\n",
        "\n",
        "\n",
        "[However, Restnet is a heavy model and training on such low data leads to very high training accuracy and very low validation accuracy ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaWxDYWYHKcn"
      },
      "source": [
        "cUSTOM cnn Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSK4y7rek7YF"
      },
      "source": [
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(17, activation='softmax')\n",
        "])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUWZB2mLlUMw",
        "outputId": "f55e979e-a38c-4cde-ea5a-dbfe8d46a890"
      },
      "source": [
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, epochs=25, steps_per_epoch=20, validation_data = validation_generator, verbose = 1, validation_steps=3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 148, 148, 64)      1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 74, 74, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 72, 72, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               3211776   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 17)                8721      \n",
            "=================================================================\n",
            "Total params: 3,480,657\n",
            "Trainable params: 3,480,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "18/20 [==========================>...] - ETA: 28s - loss: 2.7334 - accuracy: 0.1721"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 387s 18s/step - loss: 2.6858 - accuracy: 0.1803 - val_loss: 2.1899 - val_accuracy: 0.2552\n",
            "Epoch 2/25\n",
            "20/20 [==============================] - 284s 14s/step - loss: 2.0065 - accuracy: 0.2945 - val_loss: 1.4343 - val_accuracy: 0.4688\n",
            "Epoch 3/25\n",
            "20/20 [==============================] - 201s 10s/step - loss: 1.6605 - accuracy: 0.4430 - val_loss: 1.1071 - val_accuracy: 0.6875\n",
            "Epoch 4/25\n",
            "20/20 [==============================] - ETA: 0s - loss: 1.3735 - accuracy: 0.5514"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 147s 7s/step - loss: 1.3735 - accuracy: 0.5514 - val_loss: 1.1326 - val_accuracy: 0.6615\n",
            "Epoch 5/25\n",
            "20/20 [==============================] - 114s 6s/step - loss: 1.0280 - accuracy: 0.6687 - val_loss: 2.1209 - val_accuracy: 0.6510\n",
            "Epoch 6/25\n",
            "20/20 [==============================] - 74s 4s/step - loss: 0.7775 - accuracy: 0.7423 - val_loss: 0.8739 - val_accuracy: 0.8021\n",
            "Epoch 7/25\n",
            "20/20 [==============================] - 57s 3s/step - loss: 0.6727 - accuracy: 0.7992 - val_loss: 1.7370 - val_accuracy: 0.7396\n",
            "Epoch 8/25\n",
            "20/20 [==============================] - 50s 3s/step - loss: 1.0758 - accuracy: 0.7906 - val_loss: 1.2238 - val_accuracy: 0.7969\n",
            "Epoch 9/25\n",
            "20/20 [==============================] - 38s 2s/step - loss: 0.5492 - accuracy: 0.8359 - val_loss: 1.5108 - val_accuracy: 0.8125\n",
            "Epoch 10/25\n",
            "20/20 [==============================] - 33s 2s/step - loss: 0.6643 - accuracy: 0.7969 - val_loss: 2.8041 - val_accuracy: 0.7396\n",
            "Epoch 11/25\n",
            "20/20 [==============================] - 28s 1s/step - loss: 0.4946 - accuracy: 0.8516 - val_loss: 1.9018 - val_accuracy: 0.7865\n",
            "Epoch 12/25\n",
            "20/20 [==============================] - 26s 1s/step - loss: 0.5847 - accuracy: 0.8181 - val_loss: 3.1408 - val_accuracy: 0.7292\n",
            "Epoch 13/25\n",
            "20/20 [==============================] - 26s 1s/step - loss: 0.5152 - accuracy: 0.8562 - val_loss: 1.6752 - val_accuracy: 0.8385\n",
            "Epoch 14/25\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.5401 - accuracy: 0.8453 - val_loss: 2.3564 - val_accuracy: 0.7500\n",
            "Epoch 15/25\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.4010 - accuracy: 0.8744 - val_loss: 3.1632 - val_accuracy: 0.6250\n",
            "Epoch 16/25\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.4244 - accuracy: 0.8750 - val_loss: 1.1700 - val_accuracy: 0.8073\n",
            "Epoch 17/25\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.3215 - accuracy: 0.8969 - val_loss: 2.0408 - val_accuracy: 0.7604\n",
            "Epoch 18/25\n",
            "20/20 [==============================] - 19s 940ms/step - loss: 0.4078 - accuracy: 0.8766 - val_loss: 1.7368 - val_accuracy: 0.8021\n",
            "Epoch 19/25\n",
            "20/20 [==============================] - 24s 1s/step - loss: 0.4653 - accuracy: 0.8625 - val_loss: 1.8591 - val_accuracy: 0.8073\n",
            "Epoch 20/25\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.3096 - accuracy: 0.9086 - val_loss: 2.8819 - val_accuracy: 0.7500\n",
            "Epoch 21/25\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.4281 - accuracy: 0.8654 - val_loss: 1.8107 - val_accuracy: 0.8333\n",
            "Epoch 22/25\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.2802 - accuracy: 0.9062 - val_loss: 2.8670 - val_accuracy: 0.6198\n",
            "Epoch 23/25\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.4068 - accuracy: 0.8883 - val_loss: 2.8374 - val_accuracy: 0.8073\n",
            "Epoch 24/25\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.4369 - accuracy: 0.8766 - val_loss: 2.5387 - val_accuracy: 0.7812\n",
            "Epoch 25/25\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.2718 - accuracy: 0.9180 - val_loss: 3.0584 - val_accuracy: 0.7969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Z6ckXjHN2U"
      },
      "source": [
        "RESNET+MOBILE_NET+INCEPTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6eoj1lQHU0B"
      },
      "source": [
        "UNCOMMENT AND USE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCn2MaDS7ZR4",
        "outputId": "012d0e18-9281-4f5f-939c-01ca7cd08850"
      },
      "source": [
        "# from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "# resnet = tf.keras.applications.resnet.ResNet50(input_shape=(150,150,3),\n",
        "                                              #  include_top=False,\n",
        "                                              #  weights='imagenet')\n",
        "# x = tf.keras.layers.GlobalAveragePooling2D()(resnet.output)\n",
        "# x = tf.keras.layers.Flatten()(x)\n",
        "# x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "# x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "# x = tf.keras.layers.Dense(17, activation=\"softmax\", name=\"classification\")(x)\n",
        "\n",
        "# inception = InceptionV3(weights='imagenet', include_top=False)\n",
        "inception = tf.keras.applications.MobileNet(weights='imagenet', include_top=False)\n",
        "x = inception.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128,activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "predictions = Dense(17,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inception.input, outputs=predictions)\n",
        "model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "checkpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\n",
        "csv_logger = CSVLogger('history_3class.log')\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch = 20,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps= 3,\n",
        "                    epochs=25,\n",
        "                    verbose=1)\n",
        "\n",
        "# model.save('model_trained_3class.hdf5')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            " 6/20 [========>.....................] - ETA: 9s - loss: 2.2361 - accuracy: 0.4297"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 24s 1s/step - loss: 1.1756 - accuracy: 0.7129 - val_loss: 5.3650 - val_accuracy: 0.4323\n",
            "Epoch 2/25\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.3995 - accuracy: 0.9250 - val_loss: 5.4326 - val_accuracy: 0.6250\n",
            "Epoch 3/25\n",
            " 7/20 [=========>....................] - ETA: 10s - loss: 0.2927 - accuracy: 0.9509"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 21s 1s/step - loss: 0.3288 - accuracy: 0.9454 - val_loss: 4.1280 - val_accuracy: 0.5260\n",
            "Epoch 4/25\n",
            "20/20 [==============================] - 20s 987ms/step - loss: 0.2677 - accuracy: 0.9602 - val_loss: 4.2850 - val_accuracy: 0.5104\n",
            "Epoch 5/25\n",
            "20/20 [==============================] - 24s 1s/step - loss: 0.2492 - accuracy: 0.9625 - val_loss: 1.9996 - val_accuracy: 0.6094\n",
            "Epoch 6/25\n",
            "20/20 [==============================] - 23s 1s/step - loss: 0.2163 - accuracy: 0.9703 - val_loss: 2.0242 - val_accuracy: 0.5625\n",
            "Epoch 7/25\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.2180 - accuracy: 0.9742 - val_loss: 1.4381 - val_accuracy: 0.7292\n",
            "Epoch 8/25\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.1971 - accuracy: 0.9734 - val_loss: 2.3690 - val_accuracy: 0.6250\n",
            "Epoch 9/25\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.2030 - accuracy: 0.9723 - val_loss: 1.2305 - val_accuracy: 0.7552\n",
            "Epoch 10/25\n",
            "20/20 [==============================] - 24s 1s/step - loss: 0.1830 - accuracy: 0.9797 - val_loss: 1.0823 - val_accuracy: 0.8177\n",
            "Epoch 11/25\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.1448 - accuracy: 0.9844 - val_loss: 1.6317 - val_accuracy: 0.8021\n",
            "Epoch 12/25\n",
            "20/20 [==============================] - 23s 1s/step - loss: 0.1655 - accuracy: 0.9789 - val_loss: 1.3555 - val_accuracy: 0.8229\n",
            "Epoch 13/25\n",
            "20/20 [==============================] - 23s 1s/step - loss: 0.1383 - accuracy: 0.9891 - val_loss: 1.1479 - val_accuracy: 0.8385\n",
            "Epoch 14/25\n",
            "20/20 [==============================] - 19s 982ms/step - loss: 0.1330 - accuracy: 0.9845 - val_loss: 1.6076 - val_accuracy: 0.8021\n",
            "Epoch 15/25\n",
            "20/20 [==============================] - 25s 1s/step - loss: 0.1292 - accuracy: 0.9867 - val_loss: 1.4867 - val_accuracy: 0.8177\n",
            "Epoch 16/25\n",
            "20/20 [==============================] - 23s 1s/step - loss: 0.1233 - accuracy: 0.9883 - val_loss: 1.2426 - val_accuracy: 0.8385\n",
            "Epoch 17/25\n",
            "20/20 [==============================] - 20s 993ms/step - loss: 0.1378 - accuracy: 0.9853 - val_loss: 1.2931 - val_accuracy: 0.8125\n",
            "Epoch 18/25\n",
            "20/20 [==============================] - 20s 981ms/step - loss: 0.1258 - accuracy: 0.9883 - val_loss: 1.2382 - val_accuracy: 0.8333\n",
            "Epoch 19/25\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.1196 - accuracy: 0.9906 - val_loss: 1.5578 - val_accuracy: 0.8125\n",
            "Epoch 20/25\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.1116 - accuracy: 0.9906 - val_loss: 1.3843 - val_accuracy: 0.8021\n",
            "Epoch 21/25\n",
            "20/20 [==============================] - 23s 1s/step - loss: 0.0954 - accuracy: 0.9961 - val_loss: 1.3461 - val_accuracy: 0.8229\n",
            "Epoch 22/25\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.0974 - accuracy: 0.9914 - val_loss: 1.7254 - val_accuracy: 0.7865\n",
            "Epoch 23/25\n",
            "20/20 [==============================] - 20s 987ms/step - loss: 0.1079 - accuracy: 0.9883 - val_loss: 1.4848 - val_accuracy: 0.8073\n",
            "Epoch 24/25\n",
            "20/20 [==============================] - 20s 1s/step - loss: 0.0960 - accuracy: 0.9930 - val_loss: 1.6506 - val_accuracy: 0.7812\n",
            "Epoch 25/25\n",
            "20/20 [==============================] - 20s 989ms/step - loss: 0.0838 - accuracy: 0.9935 - val_loss: 1.8065 - val_accuracy: 0.7604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caPX6b_-BSN8"
      },
      "source": [
        "model.save('cfv_inception.hdf5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bXwm8j0lWmg"
      },
      "source": [
        "model.save(\"cfv.h5\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlhthSAkkw7H"
      },
      "source": [
        "Result Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "KnikqgaOlAGb",
        "outputId": "e44cfe4d-4ffd-4628-84c2-7116a5d54d05"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZfbA8e8hlNCkinRBBRTEUCKuYoG1oeuCgCJBUeyysopr76y7uhbWsj/b4ioKqICo2EDUVdQVQTqGjhAFBEQ6QoAk5/fHmUmGkDJJJpnM5HyeZ56ZuXPve987k5x559z3vq+oKs455+JLpWhXwDnnXOR5cHfOuTjkwd055+KQB3fnnItDHtydcy4OeXB3zrk45ME9jonIVBG5ItLrRpOIpInIWaVQrorIMYHHL4rI/eGsW4z9XCoinxS3ns6FS7yfe/kiIrtDntYA9gGZgefXq+rrZV+r8kNE0oBrVPWzCJerQBtVXRWpdUWkFbAGqKKqGZGop3PhqhztCriDqWqt4OOCApmIVPaA4coL/3ssfzwtEyNEpIeIrBORO0VkIzBaROqJyIcisllEtgUeNw/ZZrqIXBN4PERE/iciIwPrrhGR84q5bmsR+UpEdonIZyLynIiMy6fe4dTxbyLyTaC8T0SkYcjrg0XkRxHZIiL3FvD+nCQiG0UkIWRZXxFZFHjcTUS+FZHtIrJBRJ4Vkar5lPWqiPw95PntgW1+FpGrcq37BxGZLyI7RWStiIwIefmrwP12EdktIicH39uQ7U8RkdkisiNwf0q4700R3+f6IjI6cAzbRGRyyGt9RGRB4Bh+EJFegeUHpcBEZETwcxaRVoH01NUi8hPweWD5W4HPYUfgb6RDyPbVReSfgc9zR+BvrLqIfCQif851PItEpG9ex+rC48E9tjQG6gNHAtdhn9/owPOWwF7g2QK2PwlYDjQEHgdeFhEpxrpvAN8BDYARwOAC9hlOHQcBVwKNgKrAbQAi0h54IVB+08D+mpMHVZ0F/Ab8Ple5bwQeZwK3BI7nZOBM4E8F1JtAHXoF6nM20AbIne//DbgcqAv8ARgqIhcGXjs9cF9XVWup6re5yq4PfAT8K3BsTwIfiUiDXMdwyHuTh8Le57FYmq9DoKynAnXoBowBbg8cw+lAWn7vRx7OAI4Dzg08n4q9T42AeUBoGnEk0BU4Bfs7vgPIAl4DLguuJCJJQDPsvXHFpap+K6c37J/srMDjHsB+ILGA9TsB20KeT8fSOgBDgFUhr9UAFGhclHWxwJEB1Ah5fRwwLsxjyquO94U8/xPwceDxA8D4kNdqBt6Ds/Ip++/AK4HHtbHAe2Q+6w4H3g15rsAxgcevAn8PPH4FeDRkvbah6+ZR7tPAU4HHrQLrVg55fQjwv8DjwcB3ubb/FhhS2HtTlPcZaIIF0Xp5rPfvYH0L+vsLPB8R/JxDju2oAupQN7BOHezLZy+QlMd6icA27DwG2JfA82X9/xZvN2+5x5bNqpoefCIiNUTk34GfuTuxNEDd0NRELhuDD1R1T+BhrSKu2xTYGrIMYG1+FQ6zjhtDHu8JqVPT0LJV9TdgS377wlrp/USkGtAPmKeqPwbq0TaQqtgYqMcjWCu+MAfVAfgx1/GdJCJfBNIhO4Abwiw3WPaPuZb9iLVag/J7bw5SyPvcAvvMtuWxaQvghzDrm5fs90ZEEkTk0UBqZyc5vwAaBm6Jee0r8Dc9AbhMRCoBKdgvDVcCHtxjS+6uTbcC7YCTVPUwctIA+aVaImEDUF9EaoQsa1HA+iWp44bQsgP7bJDfyqq6BAuO53FwSgYsvbMMax0eBtxTnDpgv1xCvQG8D7RQ1TrAiyHlFtYV7WcsjRKqJbA+jHrlVtD7vBb7zOrmsd1a4Oh8yvwN+9UW1DiPdUKPcRDQB0td1cFa98E6/AqkF7Cv14BLsXTZHs2VwnJF58E9ttXGfupuD+RvHyztHQZawnOAESJSVUROBv5YSnWcBFwgIqcGTn4+ROF/s28AN2PB7a1c9dgJ7BaRY4GhYdZhIjBERNoHvlxy17821ipOD+SvB4W8thlLhxyVT9lTgLYiMkhEKovIJUB74MMw65a7Hnm+z6q6AcuFPx848VpFRILB/2XgShE5U0QqiUizwPsDsAAYGFg/GbgojDrsw35d1cB+HQXrkIWluJ4UkaaBVv7JgV9ZBIJ5FvBPvNUeER7cY9vTQHWsVTQT+LiM9nspdlJyC5bnnoD9U+el2HVU1cXAjVjA3oDlZdcVstmb2Em+z1X115Dlt2GBdxfwUqDO4dRhauAYPgdWBe5D/Ql4SER2YecIJoZsuwd4GPhGrJfO73KVvQW4AGt1b8FOMF6Qq97hKux9HgwcwH69/IKdc0BVv8NO2D4F7AC+JOfXxP1YS3sb8FcO/iWUlzHYL6f1wJJAPULdBnwPzAa2Ao9xcAwaA3TEzuG4EvKLmFyJicgEYJmqlvovBxe/RORy4DpVPTXadYkH3nJ3RSYiJ4rI0YGf8b2wPOvkwrZzLj+BlNefgFHRrku88ODuiqMx1k1vN9ZHe6iqzo9qjVzMEpFzsfMTmyg89ePC5GkZ55yLQ95yd865OBS1gcMaNmyorVq1itbunXMuJs2dO/dXVT28sPWiFtxbtWrFnDlzorV755yLSSKS+6rmPHlaxjnn4pAHd+eci0Me3J1zLg55cHfOuThUaHAXkVdE5BcRSc3ndRGRf4nIqsDsKV0iX03nnHNFEU7L/VWgVwGvn4fNvNIGmx3ohZJXyznnXEkUGtxV9StsBLf89AHGqJmJTRDQJFIVdM45V3SR6OfejINnqlkXWLYh94oich3Wuqdly9xzHjjnXIgDB2DNGli+HFasgEqVoHXrnNthh0W7huVamV7EpKqjCIz6lpyc7IPaOFfRqcKmTRa8ly/PCeTLl8Pq1ZCRkf+29evDUUcdHPCDtyOPhGrVSqfOWVmwcaN98axZA9u3Q2am1TX3fX7LBg2C004rnfoFRCK4r+fgaciaU7xpwpwrHzIyoHLULt4uXzIzYccO2LbNbtu3H3q/Y4cFvKLYtSsniO/cmbM8MRHatIGOHeGii6BdO2jb1u6zsnICavC2ejUsWADvvQf79+eUIwJNm0KTJnDEEdCo0aG34PKGDaFKlYPrt337wfsI3WdaGqSnE5aEBPtbyn3/u9/FRHB/HxgmIuOBk4AdgWm9nIsNGRkwcyZ8/LHd5s2zVmHr1tCqld1CHx95JNSsWfT97N9vgXDHDgseu3bBb7+Ff0tPh7p14fDDC77VqJH3/jMzYcsW2Ly54NvWrTmBOzTw5qVyZahTx4JWUVSvbkF88OCDA3jLlpZ+yU+DBpCcfOjyrCz4+eeDg3Famv0q2LABFi60xwcO5F1u/foW6KtVgx9/tGMPVbeu/Q20bw9/+EPOL4SjjrJtq1Q5NIBXqmRfMlFS6JC/IvIm0AObvXwTNjdjFQBVfVFEBHgW61GzB7hSVQsdNCY5OVl9bBlXKFULNMFW04EDFgjatoXatYtf7k8/wbRpFsw/+8yCWEICnHwynHqq7TMtLee2L9csgo0a5QT7Vq2gXr2coB1szQYfB5/v2RNe3UTsyyP3rVo1q1cwCOeXsqhRIyfQJybCr7/mBO38/t/r1cvZpkEDC2b16uXchz4OXVajRlQDWJGo2ufwyy/53/bssS/v3GmeevWiXftsIjJXVfP4hsu1XrTGc/fg7rLt2WMBNPdP7mArLL/WY5Mm1toLbfm1a2fBNndaJT0dvvoqJ6AvWWLLW7SAXr3s9vvfW+DKLSvLWn3BQB9sFQZvP/5orfIqVWz7unWtNVvQ4zp17MspryBevXrhATMYqAprhe/bZ2mHglr6DRocmpZw5ZYHd1f+ZGbC0qUwezbMmWO50h9+sMAZqnp1C9B5nSyrXDknVxt6Em5rSG/dKlXg6KMt0B99tO1z+nTYu9dav6efnhPQjzuu5C3PrCwLoomJsdOKdTEr3ODuZ41c6cjKglWrLIgHg/m8eTmpidq1oXNny1/mDuJHHFFwkDz++EOX/frrwcE+GPynTrUvimuvtWB+xhn556SLq1Il+0Jyrhzx4B4PMjOt9btuHaxff/D9unUW+BISoGpVa9UWdB98XK2atUQTEy1wBR/n97xKFQuowWA+d66lDcBe79wZrrkGTjzRToi1bVvwibOiatjQbqeccvByVW9NuwrJg3us2LsX5s+3wLlmzcEBfMMGC/ChqlSBZs2geXMLpFlZdjJy/367/+03O9G3f3/OstD7ffssT5273MJUqQInnAApKRbETzzRehhEq2uhB3ZXQXlwL49UYeVKmDXLbjNnWleuYO+IWrUsaDdrBmeemfM49L5hw8i0jDMyLMjv3Wv3wVvo87177cugdWsL7KV18YhzLmwe3MuDLVvgu+9ygvmsWdblDSyQd+sGt98OJ51kt8aNy65ulStbHWrVKrt9OudKzIN7pKnC7t0HX8WX35V927ZZnnrlStu2UiXo0AH697cr2E46yXpzFPUCEedchefBvaRUraX9wgvWM2Pr1sLz1HXq5FwE0qEDXHWVBfOuXUt2YY5zzgV4cC+u3bvh9dfhxRetv3atWtC3r10Uk9dVfcHHhx3mLXHnXKnz4F5UqanWSh871sYGOeEEe37ppd7qds6VGx7cw7FvH0yaZK30//3PeoMMGAA33GBjkXh3O+dcOePBvSCrV8O//w2vvGIXAh19NDzxBAwZYl0NnXOunPLgnpf58+G++2DKFMuP9+4NQ4dan/JIXlXpnHOlxIN7qJ9/tqD+6qs2RvODD9qYJM2aRbtmzjlXJB7cwS7F/+c/4bHH7PL7W2+Fe+/Ne/hX55yLARU7uGdlwbhxcM89Nk7LRRfBo49abt0552JYxU0gf/mlDWp1xRU26cPXX8Nbb3lgd87FhYoX3Fetgn79oEcPm1Zr3Di7wvTUU6NdM+eci5iKE9y3bYO//MWGn/3kE/j7321cl0sv9R4wzrm4UzFy7u+8Y71etm2Dq6+Ghx6yVIxzzsWpihHc77nHpm77/HNISop2bZxzrtTFfz4iPd2G1L3oIg/szrkKI/6D+7Jl1uUxr0mVnXMuTsV/cE9NtXsP7s65CiT+g/vixTZpc5s20a6Jc86VmfgP7qmp0K6dBXjnnKsgwgruItJLRJaLyCoRuSuP148Ukf+KyCIRmS4izSNf1WJavNhTMs65CqfQ4C4iCcBzwHlAeyBFRNrnWm0kMEZVTwAeAv4R6YoWy+7dsGaNzVPqnHMVSDgt927AKlVdrar7gfFAn1zrtAc+Dzz+Io/Xo2PJErv3lrtzroIJJ7g3A9aGPF8XWBZqIdAv8LgvUFtEGuQuSESuE5E5IjJn8+bNxalv0SxebPce3J1zFUykTqjeBpwhIvOBM4D1QGbulVR1lKomq2ry4YcfHqFdFyA1FRIToXXr0t+Xc86VI+EMP7AeaBHyvHlgWTZV/ZlAy11EagH9VXV7pCpZbKmpNlBYQkK0a+Kcc2UqnJb7bKCNiLQWkarAQOD90BVEpKGIBMu6G3glstUsJu8p45yroAoN7qqaAQwDpgFLgYmqulhEHhKR3oHVegDLRWQFcATwcCnVN3zbttnsSt5TxjlXAYU1KqSqTgGm5Fr2QMjjScCkyFathPxkqnOuAovfK1Q9uDvnKrD4De6pqVC7NrRoUfi6zjkXZ+I7uHfoACLRrolzzpW5+A3u3lPGOVeBxWdw/+UX2LzZe8o45yqs+AzufjLVOVfBxWdwD86+5C1351wFFb/BvX59aNw42jVxzrmoiM/gHjyZ6j1lnHMVVPwFd9WcbpDOOVdBxV9wX78eduzwk6nOuQot/oK795Rxzrk4DO7eU8Y55+I0uDduDA0OmeXPOecqjPgL7j7sgCuB/fth/nw7L1+eZGVZvbKyol0TFyviK7hnZVlw95SMK6JffoG//Q1atYIuXeC228pPgJ8xA046yep17rmwYUO0a+RiQXwF97Q02LPHW+4ubPPnw5VXQsuW8MAD0LEjXHYZPPkk3HFHdAP82rUwaBB07w4//wy3326B/oQT4IMPIr8/Vfj0U3tPXOyLr+DuPWVcGDIy4O234fTTrTU8cSJcdRUsWQLTpsGYMfCnP8HIkXDXXWUf4Pfsgb/+Fdq1g3ffhfvug+XL4fHHYe5caN4ceveGP/8Z9u6NzD7nzbP345xz7D0ZMsS+UFwMU9Wo3Lp27aoR98gjqqC6Y0fky3Yxb8sW1cceU23Z0v5MWrVSHTlSddu2Q9fNylIdOtTWu+sue17asrJU33xTtUUL2+/FF6uuWXPoeunpqn/5i61z/PGq339f/H1u3Kh69dWqIqqHH6764ouqd96pWrWqas2aqg8/rLp3b/HLd5EHzNEwYmx8BfdLL7X/XOdCpKaqXnedavXq9hffs6fqu++qZmQUvF1mpur119s299xTugF+zhzV7t1tX506qX75ZeHbTJ2q2qiRamKi6nPPFa1+6emqjz+uWru2apUqqrfeqrp9e87rq1apXnhhzpfgpEll8wXnClcxg3tSkur550e+XBeTFi1SPfdc+ytPTLQW6sKFRSsjM1P12mutjPvui3yA27BB9corc1rOL71U+JdOqI0bVc87z+r3xz+qbt5c8PpZWaqTJ6sefbRtc8EFqsuX57/+Z5/ZrwNQ7dFDdcGC8OsWKWlpqkOG2A9z/4KpiMH9wAH7LXn77ZEt18WczZstpVKpkmr9+hYUCgt6BcnMtC8GUH3wwcjUMT3dUkTBlvNttx3cci6KrCzVZ56xP/8mTSwg5+X771XPOsuOo3171WnTwiv/wAHV559XbdDA3tPrr1f95Zfi1bUodu9Wvf9++2KuVKlsfkHFgooX3Jcts8N57bXIlutKxeefW275z39WXbEiMmXu36/61FOqdeuqJiRY2Vu2RKbszExrYYPqiBHFL2frVtUnnsjJ+//xj5E7/gULVI87zn4F3HGH6r59tvzXX1VvvNECZL16qv/6l71Xxan7zTerVq6sWqeO6pNP5uwjkrKyVMeNU23WzN6jlBTVH3+01Fpp/YKKJRUvuE+aZIczd25ky3URt3+/BaF69azVCpZNmzat+P+0U6aoHnuslXXOOaqLF0e2zqoW4IcMsX089FDRtl28WPWGG1Rr1LDtzzgj/JZzUfz2m+0HVLt2VX30UXufExIswP/6a8n3sXSpaq9eto+2bVU//DBywXbWLNXf/S6n/v/7X85rmZmq11xjrz3wQGT2F4sqXnAfMcKaLL/9FtlyXcQ984z95b33nuWcR4xQPeIIW3bssZYC2LUrvLKWLs3JObdpo/rBB6XbqsvIUL38ctvfww8XvG5mpgW+s8+29atVU73qqrLJW7/zjqWkwFIxJelRk5+PPlJt18720bSppa4mTSpeemn9+pz3tXFj1dGj7f3LLTPT3sOS/oKKZRUvuF98seoxx0S2TBdxmzdb2uTssw8OwunpqmPHqiYn219lnTrWgyOvroCqliIYPtxSBIcdZl0aSyNFkJeMDNXLLrN6/uMfh76+Y4d9gR1zjK3TrJl9EZQk718cP/+sOn166X7Z7dunOmaM6oAB9pmBfSY9etg5he+/L3j/e/fae1Ozpp0zuOsu1Z07C95n6C+ov/41cseyfbvqd99FrrzSEtHgDvQClgOrgLvyeL0l8AUwH1gEnF9YmREP7scdp9qnT2TLdBF3ww2WIsgvbZKVpfrNN6qXXGLrVapkXfK++MJey8hQfeEFO7knYnnYTZvK9BBU1eoxaJD9Bz32mC1bsUL1ppvsJCmonnKK6vjxxctvx6IDB1S//lr17rut45pd/mXnVq6/3nrpBH+RZWVZK79VK1unb1/VH34If18ZGapXXGHb/u1vJat3Robqv/9tvZXAfhmE+8sxGiIW3IEE4AfgKKAqsBBon2udUcDQwOP2QFph5UY0uKenWyS4997IlekibsECC9Y33RTe+mvXWu+IBg3sL/WEE1Q7dtTsnPX8+aVa3UIdOGAn+8DyxCJ2DuGyy2KjBVja1q1T/c9/VPv1y/nCq1rV0kSnnWbPO3a0k+vFkZGhOnhweCmy/HzxRc4X0amnqt5yi32ObdrYtQflUSSD+8nAtJDndwN351rn38CdIevPKKzciAb3RYvsUN58M3JluojKyrKf6g0aWEqlKPbssSCRlGQ53rfeKj+9JQ4csGDeuLF1k9ywIdo1Kp/27bNAevvtqh06WJfNF16w968kMjLs2sX8UmT5Wb1atX9/265lS9UJE3L+pqZPV23e3L6oH38879x/NEUyuF8E/Cfk+WDg2VzrNAG+B9YB24Cu+ZR1HTAHmNMykleSvvGGHcqiRZEr00VUsDPT889HuyYu3uSVIsvPrl32a7BaNeu59NBD1njIbcsW+8URPCH988+lU/fiCDe4R2rgsBTgVVVtDpwPjBWRQ8pW1VGqmqyqyYcffniEdo1N0FG5so205MqdvXttCN2OHeHaa6NdGxdvEhLgtddg4EC4804b8C23rCwbEK5tW3jkEbjoIhuM7f77oXr1Q9evXx8mTYJRo+Cbb0pvJM7SFE5wXw+0CHnePLAs1NXARABV/RZIBBpGooJhWbzYPrWqVctsly58Tz5pozE/84x9BzsXaZUrw9ixMGCADY385JM5r82cCSefDFdcYSNqzpgB48bZ44KIWGNk3ryckTiHDYvcSJylLZx/tdlAGxFpjQX1gcCgXOv8BJwJvCoix2HBfXMkK1qg1FQbp9SVO+vWWUupf3/o2TPatXHxrHJleP1166Nz662wezesXGmBvEkTa91fdhlUKmK+4thj7Qvi7rvhqafgyy/hzTfL/8jihR6mqmYAw4BpwFJgoqouFpGHRKR3YLVbgWtFZCHwJjAkkBsqfXv2wOrV5f+drqDuugsyM+GJJ6JdE1cRBAN8//7w4IPw1ltwzz2wYgVcfnnRA3tQtWr2a2DqVJu1KzkZnnuu/MzWlZewfiSr6hRgSq5lD4Q8XgJ0j2zVwrR0qb3DHtzLnRkz7B/t3nuhdeto18ZVFFWqWMt69Gg4++zI/u316gWLFtlkJsOGwccfwyuvQCRPIUZK7M/ElJpq9z5varmSlQU33wxNm1rr3bmyVKUKXHdd6TQqjjgCPvoInn4aPvnE0jbPPmszfJUnsR/cFy+230xHHx3tmrgQY8bAnDnw2GNQq1a0a+NcZFWqZI2XuXOhUyeb8rBTJ5uDtryI/eCemgrHHefdMMqRnTuttf6739kEz87Fq+OPh88+s7lu9+61OWj79IFVq6Jds3gJ7p6SKVcefhg2bYJ//av4J7CcixUicOGFNsH6o4/C559D+/Zwxx3W0ImW2P7X27kT1q71k6nlyMqV1l1syBA48cRo18a5slOtml1EtXKldbkcORLatIGXX7YeY2UttoP74sV278G93LjtNvsjf+SRaNfEueho3Nh60Hz3HRxzDFxzjTV0vv66bOsR28Hde8qUK598Au+/D/fdZxeNOFeRJSfD//5n3TI3b4bTT4dLLoEffyyb/cd2cF+8GGrWhCOPjHZNKrwDB+CWW6zT0vDh0a6Nc+WDiI15s3w5jBhh49Mce6wNlVDaYju4B0+m+lm7qHvxRTuh9M9/WlrGOZejRg27Ynb5cujXz7pNlrbY7j+Ymgrnnx/tWlR4+/bZH+5ZZ9ngSs65vLVoYVdtl4XYbfL++qv1t/OTqVH3zTewbZtdyCES7do45yCWg3uwp4yfTI26adPsGjIf9dG58iP2g7u33KNu2jTo3h1q1452TZxzQbEb3FNToW5dG5nKRc2GDbBwoY2W55wrP2I7uHfo4EneKPvkE7s/99zo1sM5d7DYDO6qlpbxlEzUTZsGjRpBUlK0a+KcCxWbwX3jRti61YN7lGVl2RCn55zjlxo4V97E5r+kDztQLsybZz1SPSXjXPkTm8Hde8qE7bvvbIyLLVsiX/a0aXZ/zjmRL9s5VzKxGdxTUy3RWx4nLixn3nnHZouZODHyZU+bBp0720fhnCtfYje4e0omLLNm2f2bb0a23J074dtvPSXjXHkVe8Hde8qELTMTZs+2QYu+/trmNYmUzz+3CYE9uDtXPsVecP/pJ9i924N7GBYvht9+s+m+ACZMiFzZ06bZxNennBK5Mp1zkRN7wd17yoRt5ky7v+wyO6kaqdSMKnz8Mfz+91C1amTKdM5FVuwFdx8wLGwzZ0LDhnDUUZCSYl0XV6woebkrV0JamqdknCvPYi+4p6TA5Mk2rowr0KxZcNJJNkLDJZfY/fjxJS832AXSg7tz5VdYwV1EeonIchFZJSJ35fH6UyKyIHBbISLbI1/VgBYtoE+fUis+XuzYAUuXwu9+Z8+bNbM5HN9809IqJTFtmk2nd/TRJa+nc650FBrcRSQBeA44D2gPpIhI+9B1VPUWVe2kqp2A/wPeKY3KuvDNnm1B/KSTcpalpMCyZTaKY3Ht2wdffOGtdufKu3Ba7t2AVaq6WlX3A+OBgprOKUCEe1W7opo509Iw3brlLOvf3ybVKMmJ1W++gT17PLg7V96FE9ybAaE9pNcFlh1CRI4EWgOf5/P6dSIyR0TmbN68uah1dUUwa5bNsl6nTs6yhg3h7LMt756VVbxyfdYl52JDpE+oDgQmqWpmXi+q6ihVTVbV5MN96IBSo2ot92C+PVRKil0q8O23xSt72jQ49VSfdcm58i6c4L4eaBHyvHlgWV4G4imZqFuzxkZrzCu4X3ghJCYWLzUTnHXJUzLOlX/hBPfZQBsRaS0iVbEA/n7ulUTkWKAeUMw2oYuU4MVLoSdTg2rXhgsugLfesuEDisJnXXIudhQa3FU1AxgGTAOWAhNVdbGIPCQivUNWHQiMVy1pRztXUjNnQs2a+V/nlZICv/xivV6Kwmddci52VA5nJVWdAkzJteyBXM9HRK5ariRmzbLhBirn8+mefz4cdpilZs4+O7wyg7Mu9erlsy45Fwv83zTOpKfD/Pl559uDEhOhb18b633fvvDK9VmXnIstHtzjzIIFcOBA3vn2UCkpdhXr1KnhleuzLjkXWzy4x5mCTqaG+v3vrd97uL1mfNYl52KLB/c4M2sWtMAFV/4AABk7SURBVGwJTZsWvF6VKnDxxfDBBzY8fkF27PBZl5yLNR7c48zMmYW32oNSUmDvXnjvvYLXC8661KtXyevnnCsbHtzjyKZNNs56QSdTQ3XvDs2bF56aCc66dPLJJa6ic66MeHCPI8HJsMNtuVeqBAMHWvDeujXvdVTtdZ91ybnY4sE9jsycaX3bu3QJf5uUFEu5vP123q/7rEvOxSYP7nFk1iy7erR69fC36dwZ2rbNPzXjsy45F5s8uMeJzEz47rvw8+1BItZ6nz4dfv750Nd91iXnYpMH9zixdKl1aQw33x5q4EDLrU+cePByn3XJudjlwT1OBC9eKmrLHWxSj06dDk3N+KxLzsUuD+5xYtYsqF8fjjmmeNunpFha54cfcpZNm2YXO/msS87FHg/ucSJ48ZJI8bYfONDux4/PWfbxx9YX3mddci72eHCPA7t2weLFxUvJBLVsaYE8GNw3bIBFizwl41ys8uAeB2bPthOixTmZGiolBVJT7eazLjkX2zy4x4HgydRu3UpWzsUXQ0KCnVj1WZeci21hzcTkyrdZs6BdO6hXr2TlNGoEZ55pwX3XLp91yblY5v+6MU7VWu4lybeHSkmBNWt81iXnYl2FCO5Ll9oYKfHoxx9tsutIBfcLL8wZIMxnXXIudlWItMyll9rUc4sWFb+rYHkV7sxL4apbFy65BH76yWddci6WxX1w37cPvv/eRj5csMAGyoons2bZQGEdO0auzNGjLd3jnItdcZ+WWbLEAjvAmDHRrUtpmDkTkpNtqN9ISUiIbHnOubIX98F9wQK7T0qCN97ICfTxYN8+mD8/cvl251z8qBDBvWZNeOABO/EYvDgnHixcaAE+Uvl251z8qBDBPSkJLrjABtYqT6mZ+++Ha66BrKzibV+SkSCdc/EtrOAuIr1EZLmIrBKRu/JZZ4CILBGRxSLyRmSrWTxZWRbcO3Wy7n0DB8J778GOHdGuGWzfDk88AS+/DI88UrwyZs2yCa6bNYts3Zxzsa/Q4C4iCcBzwHlAeyBFRNrnWqcNcDfQXVU7AMNLoa5FlpYGO3dacAe4/HJIT4dJk6JaLcDqsG8fnHyypYw+/bToZQRHgnTOudzCabl3A1ap6mpV3Q+MB/rkWuda4DlV3Qagqr9EtprFEzyZGgzu3brZfKFjx0avTkFjxtgkGZ9+Cu3bw6BBsHZt+Ntv3gyrV3tKxjmXt3CCezMgNOysCywL1RZoKyLfiMhMEemVV0Eicp2IzBGROZs3by5ejYtgwQIbG+X444P7h8GD4csvrVUfLWvWwNdfW11q1oS337ZW/IABsH9/eGXMmmX33nJ3zuUlUidUKwNtgB5ACvCSiNTNvZKqjlLVZFVNPvzwwyO06/wtWGCt4+rVc5Zddpndv/56qe8+X+PGHVyXdu3glVcszXLbbeGVMWuW9Ufv2rV06uici23hBPf1QIuQ580Dy0KtA95X1QOqugZYgQX7qAqeTA3VqhWcfrqlRaJxFaaqpYV69LAJMoIuughuuQX+7/8Ong0pPzNnwgknQI0apVZV51wMCye4zwbaiEhrEakKDATez7XOZKzVjog0xNI0qyNYzyLbssVy2HkNN3D55bBihU1yUdZmzbJBzC6//NDXHnvMZkO65hq7sjY/WVk236nn251z+Sk0uKtqBjAMmAYsBSaq6mIReUhEegdWmwZsEZElwBfA7aq6pbQqHY6FC+0+d8sdrJWcmBidPu9jx1qaqH//Q1+rUgUmTrQ8fP/+NqZ6XpYts15AHtydc/kJK+euqlNUta2qHq2qDweWPaCq7wceq6r+RVXbq2pHVQ0jsVC6QocdyK1OHejTxyalCPcEZiTs22cplwsvhMMOy3udpk1tnRUr4Npr804dRXokSOdc/InbK1QXLLCLe/I7b3v55bB1K0ydWnZ1mjLF9plXSiZUz552YdOECZaDz23WLJt1qU3Uz2o458qruA7ueaVkgs45x8YrL8vUzNixcMQRcNZZha97xx3Quzfceit8++3Br82caX32fQo851x+4jI8pKfb7EsFBffKle3CoQ8/tNZ0aduyxfZ16aXhDacrAq+9Zj1qLr7YBj0D2L0bUlM93+6cK1hcBvfgGO4FBXew9Mj+/XYSs7RNnGizQQ0eHP42devaBU5bttgXUWYmzJljvWU83+6cK0hcBvfcww7kp1Mn6NChbIYjGDPGZkvK6wRvQTp1guefh//+Fx58MOfK1G7dIl9H51z8iNvgXqsWHHVUweuJWOt9xgxYtar06rNypeXJL7+8eHO4XnklXH01PPwwvPiinUht0CDy9XTOxY+4De5JSeGdcBw0yAJuabbex461ugwaVPwy/u//7IKstDTPtzvnChd3wT10DPdwNG8OZ55pAbg0hiPIyrKyzzrL+rAXV/XqNkxwy5Y28YhzzhUk7oJ7Wppd2RlucAc7yblmDXzzTeTr8803VqeinEjNz1FHWVkDBpS8LOdcfIu74B7uydRQ/frZAFylkZoZM8aGE+jbNzLlFSdn75yreOIuuM+fb0PhdugQ/ja1atlYLhMmWB/5SNm7F956y8quWTNy5TrnXGHiLrjnNYZ7OAYPtrlVP/wwcnX54AMrs7DhBpxzLtLiMrgXJSUT9Pvf2wnPSA5HMGaMnbDt0SNyZTrnXDjiKrj/+iusW1e84J6QYEMDTJ1q85OW1C+/wMcfW5kJCSUvzznniiKugntBY7iH4/LLbdiCcGZCKsybb9pwAZHoJeOcc0UVV8G9oDHcw3H88XahUCRSM2PHQpcuRTux65xzkRJ3wb2gMdzDMXiwDc61dGnxy1iyBObO9ROpzrnoibvgntecqUWRkmI58pL0eR871spISSlZXZxzrrjiJriHM4Z7OBo3tok8xo2zoQOKKjPTtu3VyyYDcc65aIib4L54sQXWkgZ3sHTK2rXw+OPw229F23b6dOux4ykZ51w0xU1wL86wA/np0wdOOw3uvtv6qd9+O/z4Y3jbjh1rk1//8Y8lr4dzzhVXXAX32rWhdeuSl1W9Onz5pQ36dc458NRTNmhX//62PL/RI3/7zUZuHDCg6FfIOudcJMVVcA93DPdwiMApp9h4M2vWwJ13WmDv0cNO2r7yyqHj0EyebAHe+7Y756ItLoJ7VpZdwBSJlExeWrSARx6xPPx//mP7u/pqW37vvbB+va03Zgy0agWnnlo69XDOuXDFRXBfs6boY7gXR/XqFtQXLoTPP7cg/o9/WEC/+GL47DO47LLI/XpwzrniqhztCkRCJE+mhkMEeva025o18Nxz1qIHT8m44jlw4ADr1q0jPZJjTruYlpiYSPPmzalSpUqxtg8ruItIL+AZIAH4j6o+muv1IcATQCBBwbOq+p9i1agYFiwo+hjukdK6NYwcCSNGWNqmbduyr4OLfevWraN27dq0atUK8RlZKjxVZcuWLaxbt47WxewlUmgCQUQSgOeA84D2QIqItM9j1Qmq2ilwK7PADhbcjzsOEhPLcq8Hq1XL6uBccaSnp9OgQQMP7A4AEaFBgwYl+iUXTna4G7BKVVer6n5gPNCn2HssBcUdw9258sQDuwtV0r+HcIJ7M2BtyPN1gWW59ReRRSIySURa5FWQiFwnInNEZM7mSAyaTsnGcHfOuXgVqX4dHwCtVPUE4FPgtbxWUtVRqpqsqsmHl2ToxhBlfTLVuXi0ZcsWOnXqRKdOnWjcuDHNmjXLfr5///4Ct50zZw433XRTofs45ZRTIlVdF4ZwTqiuB0Jb4s3JOXEKgKpuCXn6H+DxklctPCUdw905Bw0aNGBB4J9pxIgR1KpVi9tuuy379YyMDCpXzjtcJCcnk5ycXOg+ZsyYEZnKlqHMzEwSYnQqtXCC+2ygjYi0xoL6QGBQ6Aoi0kRVNwSe9gZKMBp60SxYYOO/NGxYVnt0rpQNH57TaomUTp3g6aeLtMmQIUNITExk/vz5dO/enYEDB3LzzTeTnp5O9erVGT16NO3atWP69OmMHDmSDz/8kBEjRvDTTz+xevVqfvrpJ4YPH57dqq9Vqxa7d+9m+vTpjBgxgoYNG5KamkrXrl0ZN24cIsKUKVP4y1/+Qs2aNenevTurV6/mw1yz1qelpTF48GB+C4zq9+yzz2b/KnjssccYN24clSpV4rzzzuPRRx9l1apV3HDDDWzevJmEhATeeust1q5dm11ngGHDhpGcnMyQIUNo1aoVl1xyCZ9++il33HEHu3btYtSoUezfv59jjjmGsWPHUqNGDTZt2sQNN9zA6tWrAXjhhRf4+OOPqV+/PsOHDwfg3nvvpVGjRtx8883F/+yKqdDgrqoZIjIMmIZ1hXxFVReLyEPAHFV9H7hJRHoDGcBWYEgp1vkgfjLVudKzbt06ZsyYQUJCAjt37uTrr7+mcuXKfPbZZ9xzzz28/fbbh2yzbNkyvvjiC3bt2kW7du0YOnToIX2158+fz+LFi2natCndu3fnm2++ITk5meuvv56vvvqK1q1bk5LPhAiNGjXi008/JTExkZUrV5KSksKcOXOYOnUq7733HrNmzaJGjRps3boVgEsvvZS77rqLvn37kp6eTlZWFmvXrs2z7KAGDRowb948wFJW1157LQD33XcfL7/8Mn/+85+56aabOOOMM3j33XfJzMxk9+7dNG3alH79+jF8+HCysrIYP3483333XZHf90gIq5+7qk4BpuRa9kDI47uBuyNbtcLt3QvLlkHfvmW9Z+dKURFb2KXp4osvzk5L7NixgyuuuIKVK1ciIhw4cCDPbf7whz9QrVo1qlWrRqNGjdi0aRPNmzc/aJ1u3bplL+vUqRNpaWnUqlWLo446Krtfd0pKCqNGjTqk/AMHDjBs2DAWLFhAQkICK1asAOCzzz7jyiuvpEaNGgDUr1+fXbt2sX79evoGgkRimP2lL7nkkuzHqamp3HfffWzfvp3du3dz7rnnAvD5558zJjAnZ0JCAnXq1KFOnTo0aNCA+fPns2nTJjp37kyDBg3C2mekxfQVqpEcw905d6iaNWtmP77//vvp2bMn7777LmlpafTo0SPPbapVq5b9OCEhgYyMjGKtk5+nnnqKI444goULF5KVlRV2wA5VuXJlskJm48ndnzz0uIcMGcLkyZNJSkri1VdfZfr06QWWfc011/Dqq6+yceNGrrrqqiLXLVJiehSUYFqypFPrOecKt2PHDpo1s17Qr776asTLb9euHatXryYtLQ2ACRMm5FuPJk2aUKlSJcaOHUtmZiYAZ599NqNHj2bPnj0AbN26ldq1a9O8eXMmT54MwL59+9izZw9HHnkkS5YsYd++fWzfvp3//ve/+dZr165dNGnShAMHDvD6669nLz/zzDN54YUXADvxumPHDgD69u3Lxx9/zOzZs7Nb+dEQ88H9sMNs4C7nXOm64447uPvuu+ncuXORWtrhql69Os8//zy9evWia9eu1K5dmzp16hyy3p/+9Cdee+01kpKSWLZsWXYru1evXvTu3Zvk5GQ6derEyJEjARg7diz/+te/OOGEEzjllFPYuHEjLVq0YMCAARx//PEMGDCAzgW0EP/2t79x0kkn0b17d4499tjs5c888wxffPEFHTt2pGvXrixZsgSAqlWr0rNnTwYMGBDVnjai+c08UcqSk5N1zpw5JSrj1FNtBMavvopQpZyLkqVLl3Kcj1/B7t27qVWrFqrKjTfeSJs2bbjllluiXa0iycrKokuXLrz11lu0adOmRGXl9XchInNVtdC+pzHbci/tMdydc2XvpZdeolOnTnTo0IEdO3Zw/fXXR7tKRbJkyRKOOeYYzjzzzBIH9pKK2ROqq1fD7t0e3J2LJ7fcckvMtdRDtW/fPrvfe7TFbMvdhx1wzrn8xXRwr1wZ2uc1+LBzzlVwMR3coz2Gu3POlVcxHdw9JeOcc3mLyeC+eTOsX+/B3blI6dmzJ9OmTTto2dNPP83QoUPz3aZHjx4EuzOff/75bN++/ZB1RowYkd3fPD+TJ0/O7iMO8MADD/DZZ58VpfouDzEZ3BcutHsP7s5FRkpKCuPHjz9o2fjx4/MdvCu3KVOmULdu3WLtO3dwf+ihhzjrrLOKVVa0BK+SLU9iMrj7GO4ung0fDj16RPYWGIE2XxdddBEfffRR9sQcaWlp/Pzzz5x22mkMHTqU5ORkOnTowIMPPpjn9q1ateLXX38F4OGHH6Zt27aceuqpLF++PHudl156iRNPPJGkpCT69+/Pnj17mDFjBu+//z633347nTp14ocffmDIkCFMmjQJgP/+97907tyZjh07ctVVV7Fv377s/T344IN06dKFjh07smzZskPqlJaWxmmnnUaXLl3o0qXLQePJP/bYY3Ts2JGkpCTuuusuAFatWsVZZ51FUlISXbp04YcffmD69OlccMEF2dsNGzYse+iFVq1aceedd2ZfsJTX8QFs2rSJvn37kpSURFJSEjNmzOCBBx7g6ZAB4u69916eeeaZgj+kIorJ4D5/PrRoAVEabM25uFO/fn26devG1KlTAWu1DxgwABHh4YcfZs6cOSxatIgvv/ySRYsW5VvO3LlzGT9+PAsWLGDKlCnMnj07+7V+/foxe/ZsFi5cyHHHHcfLL7/MKaecQu/evXniiSdYsGABRx99dPb66enpDBkyhAkTJvD999+TkZGRPZYLQMOGDZk3bx5Dhw7NM/UTHBp43rx5TJgwIXtc+dChgRcuXMgdd9wB2NDAN954IwsXLmTGjBk0adKk0PctODTwwIED8zw+IHto4IULFzJv3jw6dOjAVVddlT2iZHBo4Msuu6zQ/RVFTF7E5CdTXTyL1oi/wdRMnz59GD9+fHZwmjhxIqNGjSIjI4MNGzawZMkSTjjhhDzL+Prrr+nbt2/2sLu9e/fOfi2/oXPzs3z5clq3bk3btm0BuOKKK3juueeyJ8Lo168fAF27duWdd945ZPuKPjRwzAX34Bju/ftHuybOxZc+ffpwyy23MG/ePPbs2UPXrl1Zs2YNI0eOZPbs2dSrV48hQ4YcMjxuuIo6dG5hgsMG5zdkcEUfGjjm0jKpqTaujLfcnYusWrVq0bNnT6666qrsE6k7d+6kZs2a1KlTh02bNmWnbfJz+umnM3nyZPbu3cuuXbv44IMPsl/Lb+jc2rVrs2vXrkPKateuHWlpaaxatQqw0R3POOOMsI+nog8NHHPB3YcdcK70pKSksHDhwuzgnpSUROfOnTn22GMZNGgQ3bt3L3D7Ll26cMkll5CUlMR5553HiSeemP1afkPnDhw4kCeeeILOnTvzww8/ZC9PTExk9OjRXHzxxXTs2JFKlSpxww03hH0sFX1o4Jgb8ve992D0aHjnHRvu17l44EP+VjzhDA1coYb87dMHJk/2wO6ci11lMTRwzJ1Qdc65WFcWQwN7+9e5ciJaKVJXPpX078GDu3PlQGJiIlu2bPEA7wAL7Fu2bClW980gT8s4Vw40b96cdevWsXnz5mhXxZUTiYmJNG/evNjbe3B3rhyoUqUKrVu3jnY1XBzxtIxzzsUhD+7OOReHPLg751wcitoVqiKyGfixmJs3BH6NYHViTUU+/op87FCxj9+P3RypqocXtkHUgntJiMiccC6/jVcV+fgr8rFDxT5+P/aiHbunZZxzLg55cHfOuTgUq8F9VLQrEGUV+fgr8rFDxT5+P/YiiMmcu3POuYLFasvdOedcATy4O+dcHIq54C4ivURkuYisEpG7ol2fsiQiaSLyvYgsEJGiT2MVY0TkFRH5RURSQ5bVF5FPRWRl4L5eNOtYWvI59hEisj7w+S8QkfOjWcfSIiItROQLEVkiIotF5ObA8ory2ed3/EX6/GMq5y4iCcAK4GxgHTAbSFHVJVGtWBkRkTQgWVUrxIUcInI6sBsYo6rHB5Y9DmxV1UcDX+71VPXOaNazNORz7COA3ao6Mpp1K20i0gRooqrzRKQ2MBe4EBhCxfjs8zv+ARTh84+1lns3YJWqrlbV/cB4oE+U6+RKiap+BWzNtbgP8Frg8WvYH33cyefYKwRV3aCq8wKPdwFLgWZUnM8+v+MvklgL7s2AtSHP11GMg45hCnwiInNF5LpoVyZKjlDVDYHHG4EjolmZKBgmIosCaZu4TEuEEpFWQGdgFhXws891/FCEzz/WgntFd6qqdgHOA24M/HSvsNRyirGTVyy5F4CjgU7ABuCf0a1O6RKRWsDbwHBV3Rn6WkX47PM4/iJ9/rEW3NcDLUKeNw8sqxBUdX3g/hfgXSxNVdFsCuQkg7nJX6JcnzKjqptUNVNVs4CXiOPPX0SqYIHtdVV9J7C4wnz2eR1/UT//WAvus4E2ItJaRKoCA4H3o1ynMiEiNQMnVxCRmsA5QGrBW8Wl94ErAo+vAN6LYl3KVDCwBfQlTj9/ERHgZWCpqj4Z8lKF+OzzO/6ifv4x1VsGIND952kgAXhFVR+OcpXKhIgchbXWwaZHfCPej11E3gR6YMOdbgIeBCYDE4GW2JDRA1Q17k485nPsPbCf5AqkAdeH5KDjhoicCnwNfA9kBRbfg+WdK8Jnn9/xp1CEzz/mgrtzzrnCxVpaxjnnXBg8uDvnXBzy4O6cc3HIg7tzzsUhD+7OOReHPLg751wc8uDunHNx6P8BDWAT6/KSO+IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VblGs4BSC4iT",
        "outputId": "c94eb917-c596-420e-c43c-d454de750c5c"
      },
      "source": [
        "from tensorflow.python import keras\n",
        "import tensorflow\n",
        "\n",
        "modelnew = keras.models.load_model('/content/cfv.h5')\n",
        "converter=tensorflow.lite.TFLiteConverter.from_keras_model(modelnew)\n",
        "tflite_model = converter.convert()\n",
        "open(\"cfv_tf.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpi5yhzdr1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13927360"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z086ZoL6lZ7E",
        "outputId": "d6496d2b-9168-446d-ed51-216a1c405b69"
      },
      "source": [
        "from tensorflow.python import keras\n",
        "import tensorflow\n",
        "\n",
        "modelnew = keras.models.load_model('/content/cfv_inception.hdf5')\n",
        "converter=tensorflow.lite.TFLiteConverter.from_keras_model(modelnew)\n",
        "tflite_model = converter.convert()\n",
        "open(\"cfv_inception_tf.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpczs7pbq0/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpczs7pbq0/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88187360"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}